{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8932f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minták: 167496 | arány: {0: 0.729, 1: 0.271}\n",
      "Top CWE-k:\n",
      " cwe\n",
      "CWE-074    42081\n",
      "CWE-190    28479\n",
      "CWE-191    23005\n",
      "CWE-089    15009\n",
      "CWE-369    12504\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, pandas as pd\n",
    "\n",
    "# ---- Állítsd be: ----\n",
    "DATASET_DIR = Path(r\"C:\\Szakdolgozat\\SARD\\JAVA\")\n",
    "LANG = 'java'\n",
    "SELECT_CWES = None  # pl. ['CWE-079','CWE-089'] vagy None = mind\n",
    "\n",
    "LANG_CFG = {\n",
    "    'java': {'subdir': 'JAVA', 'exts': ['.java', '.java.txt', '.txt']},\n",
    "}\n",
    "\n",
    "def _read_text_file(p: Path) -> str:\n",
    "    for enc in ('utf8','latin-1','cp1252'):\n",
    "        try:\n",
    "            return p.read_text(encoding=enc, errors='ignore')\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ''\n",
    "\n",
    "_CWE_RE = re.compile(r'(?:^|[\\\\/])(CWE[-_]?(\\d{1,3}))(?:$|[\\\\/])', re.I)\n",
    "def _canonical_cwe(path_str: str) -> str | None:\n",
    "    m = _CWE_RE.search(path_str)\n",
    "    return f\"CWE-{m.group(2).zfill(3)}\" if m else None\n",
    "\n",
    "def _label_from_path_or_name(p: Path) -> int | None:\n",
    "    s = p.as_posix().lower()\n",
    "    if '/bad/' in s or '\\\\bad\\\\' in s:   return 1\n",
    "    if '/good/' in s or '\\\\good\\\\' in s: return 0\n",
    "    n = p.name.lower()\n",
    "    if 'bad' in n:  return 1\n",
    "    if 'good' in n: return 0\n",
    "    return None\n",
    "\n",
    "def _looks_like_code_java(txt: str) -> bool:\n",
    "    t = txt.strip()\n",
    "    if len(t) < 20: return False\n",
    "    keys = ('class ', 'interface ', 'enum ', 'package ', 'import ',\n",
    "            'public ', 'private ', 'protected ', 'void ', 'static ')\n",
    "    return any(k in t for k in keys) and ('{' in t or ';' in t)\n",
    "\n",
    "def load_sard_java(root: Path, select_cwes=None) -> pd.DataFrame:\n",
    "    cfg = LANG_CFG['java']\n",
    "    base = root if root.name.upper()==cfg['subdir'] else ((root/cfg['subdir']) if (root/cfg['subdir']).exists() else root)\n",
    "    sel  = {c.upper() for c in select_cwes} if select_cwes else None\n",
    "\n",
    "    rows = []\n",
    "    for cwe_dir in base.iterdir():\n",
    "        if not cwe_dir.is_dir(): \n",
    "            continue\n",
    "        cwe = _canonical_cwe(cwe_dir.as_posix())\n",
    "        if not cwe or (sel and cwe.upper() not in sel):\n",
    "            continue\n",
    "        for ext in cfg['exts']:\n",
    "            for p in cwe_dir.rglob(f\"*{ext}\"):\n",
    "                s = p.as_posix().lower()\n",
    "                if 'testcasesupport' in s:\n",
    "                    continue\n",
    "                lbl = _label_from_path_or_name(p)\n",
    "                if lbl is None:\n",
    "                    continue\n",
    "                code = _read_text_file(p)\n",
    "                if not code.strip():\n",
    "                    continue\n",
    "                if p.suffix.lower()=='.txt' and not p.name.lower().endswith('.java.txt'):\n",
    "                    # sima .txt → szűrés heurisztikával\n",
    "                    if not _looks_like_code_java(code):\n",
    "                        continue\n",
    "                rows.append({'code': code, 'label': int(lbl), 'cwe': cwe, 'path': p.as_posix()})\n",
    "    if not rows:\n",
    "        raise RuntimeError(f\"Nem találtam mintát itt: {base}\")\n",
    "    df = pd.DataFrame(rows, columns=['code','label','cwe','path']).dropna().reset_index(drop=True)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    print(\"Minták:\", len(df), \"| arány:\", df['label'].value_counts(normalize=True).round(3).to_dict())\n",
    "    print(\"Top CWE-k:\\n\", df['cwe'].value_counts().head())\n",
    "    return df\n",
    "\n",
    "raw_df = load_sard_java(DATASET_DIR, SELECT_CWES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b0f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 133996 | osztályarány: {0: 0.729, 1: 0.271}\n",
      "Val: 16750 | osztályarány: {0: 0.729, 1: 0.271}\n",
      "Test: 16750 | osztályarány: {0: 0.729, 1: 0.271}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_tmp = train_test_split(raw_df, test_size=0.2, stratify=raw_df['label'], random_state=42)\n",
    "df_val, df_test  = train_test_split(df_tmp,  test_size=0.5, stratify=df_tmp['label'], random_state=42)\n",
    "for name, df in [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]:\n",
    "    print(f\"{name}: {len(df)} | osztályarány:\", df['label'].value_counts(normalize=True).round(3).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de34d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree-Sitter OK (Java)\n"
     ]
    }
   ],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java as tsjava\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "TS_LANG = Language(tsjava.language())\n",
    "parser = Parser(TS_LANG)\n",
    "print(\"Tree-Sitter OK (Java)\")\n",
    "\n",
    "EDGE_TYPES = {'parent': 0, 'next_sibling': 1, 'next_token': 2}\n",
    "\n",
    "@dataclass\n",
    "class ASTGraph:\n",
    "    nodes: List[Dict[str, Any]]\n",
    "    edges: List[Tuple[int, int, str]]\n",
    "    label: int\n",
    "    raw: str\n",
    "\n",
    "def build_augmented_ast(code: str) -> ASTGraph:\n",
    "    tree = parser.parse(code.encode('utf8'))\n",
    "    nodes, edges = [], []\n",
    "    nid, max_depth = 0, 0\n",
    "\n",
    "    def walk(node, depth=0, parent_id=None, last_sib_id=None):\n",
    "        nonlocal nid, max_depth\n",
    "        my = nid; nid += 1\n",
    "        max_depth = max(max_depth, depth)\n",
    "        text = code.encode('utf8')[node.start_byte:node.end_byte].decode('utf8', 'ignore')\n",
    "        children = node.children\n",
    "        nodes.append({\n",
    "            'id': my,\n",
    "            'type': node.type,\n",
    "            'text': text,\n",
    "            'is_leaf': int(len(children)==0),\n",
    "            'depth': depth\n",
    "        })\n",
    "        if parent_id is not None:\n",
    "            edges.append((parent_id, my, 'parent'))\n",
    "        if last_sib_id is not None:\n",
    "            edges.append((last_sib_id, my, 'next_sibling'))\n",
    "        prev = None\n",
    "        for ch in children:\n",
    "            ch_id = walk(ch, depth+1, my, prev)\n",
    "            if prev is not None:\n",
    "                edges.append((prev, ch_id, 'next_token'))\n",
    "            prev = ch_id\n",
    "        return my\n",
    "\n",
    "    walk(tree.root_node)\n",
    "    md = max(1, max_depth)\n",
    "    for n in nodes:\n",
    "        n['depth_n'] = n['depth'] / md\n",
    "    return ASTGraph(nodes=nodes, edges=edges, label=-1, raw=code)\n",
    "\n",
    "def df_to_graphs(df):\n",
    "    out = []\n",
    "    for _, row in df.iterrows():\n",
    "        g = build_augmented_ast(str(row['code']))\n",
    "        g.label = int(row['label'])\n",
    "        out.append(g)\n",
    "    return out\n",
    "\n",
    "graphs_train = df_to_graphs(df_train)\n",
    "graphs_val   = df_to_graphs(df_val)\n",
    "graphs_test  = df_to_graphs(df_test)\n",
    "len(graphs_train), len(graphs_val), len(graphs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aeb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, numpy as np, torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "TOK_DIM = 1024\n",
    "TOK_SENTINEL = TOK_DIM  # +1 dim a sentinelnek\n",
    "\n",
    "def _hash_bucket(s: str, D: int = TOK_DIM) -> int:\n",
    "    if not s or not s.strip():\n",
    "        return TOK_SENTINEL\n",
    "    h = hashlib.md5(s.strip().encode('utf8')).hexdigest()\n",
    "    return int(h, 16) % D\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self): self.map = {}\n",
    "    def id(self, k):\n",
    "        if k not in self.map: self.map[k] = len(self.map)\n",
    "        return self.map[k]\n",
    "    def size(self): return len(self.map)\n",
    "\n",
    "type_vocab = Vocab()\n",
    "for g in graphs_train:\n",
    "    for n in g.nodes:\n",
    "        type_vocab.id(n['type'])\n",
    "vocab_size = type_vocab.size(); print(\"vocab_size =\", vocab_size)\n",
    "\n",
    "def to_pyg(g: ASTGraph) -> Data:\n",
    "    type_ids, tok_ids, small = [], [], []\n",
    "    max_depth = max(1, max(n['depth'] for n in g.nodes) if g.nodes else 1)\n",
    "    for n in g.nodes:\n",
    "        type_ids.append([type_vocab.id(n['type'])])\n",
    "        tok_ids.append([_hash_bucket(n['text']) if n['is_leaf'] else TOK_SENTINEL])\n",
    "        small.append([float(n['is_leaf']), float(n['depth'])/max_depth])\n",
    "\n",
    "    x_type  = torch.tensor(np.array(type_ids), dtype=torch.long)\n",
    "    x_tok   = torch.tensor(np.array(tok_ids),  dtype=torch.long)\n",
    "    x_small = torch.tensor(np.array(small),    dtype=torch.float)\n",
    "\n",
    "    if not g.edges:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_type  = torch.empty((0,),  dtype=torch.long)\n",
    "    else:\n",
    "        src = [s for s,_,_ in g.edges]\n",
    "        dst = [d for _,d,_ in g.edges]\n",
    "        et  = [EDGE_TYPES[t] for *_,t in g.edges]\n",
    "        edge_index = torch.tensor([src,dst], dtype=torch.long)\n",
    "        edge_type  = torch.tensor(et, dtype=torch.long)\n",
    "\n",
    "    data = Data(\n",
    "        x_type=x_type, x_tok=x_tok, x_small=x_small,\n",
    "        edge_index=edge_index, y=torch.tensor([g.label], dtype=torch.long)\n",
    "    )\n",
    "    data.edge_type = edge_type\n",
    "    data.x = x_type.clone()  # kompat, ha valahol .x-re hivatkozol\n",
    "    return data\n",
    "\n",
    "pyg_train = [to_pyg(g) for g in graphs_train]\n",
    "pyg_val   = [to_pyg(g) for g in graphs_val]\n",
    "pyg_test  = [to_pyg(g) for g in graphs_test]\n",
    "print(\"PyG gráfok:\", len(pyg_train), len(pyg_val), len(pyg_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "train_loader = DataLoader(pyg_train, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(pyg_val,   batch_size=128)\n",
    "test_loader  = DataLoader(pyg_test,  batch_size=128)\n",
    "\n",
    "y_train = np.array([int(g.y.item()) for g in pyg_train])\n",
    "pos = (y_train==1).sum(); neg = (y_train==0).sum()\n",
    "class_weight = torch.tensor([1.0, max(1.0, neg/max(1,pos))], dtype=torch.float)\n",
    "num_edge_types = 3  # parent / next_sibling / next_token\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GatedGraphConv, global_mean_pool\n",
    "\n",
    "class GGNNBlockFeats(nn.Module):\n",
    "    def __init__(self, channels: int, steps: int, num_edge_types: int = 3):\n",
    "        super().__init__()\n",
    "        self.num_edge_types = max(1, num_edge_types)\n",
    "        self.convs = nn.ModuleList([GatedGraphConv(channels, num_layers=steps)\n",
    "                                    for _ in range(self.num_edge_types)])\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "    def forward(self, h, edge_index, edge_type=None):\n",
    "        if (edge_type is None) or (self.num_edge_types == 1):\n",
    "            h_msg = self.convs[0](h, edge_index)\n",
    "        else:\n",
    "            parts = []\n",
    "            for t, conv in enumerate(self.convs):\n",
    "                mask = (edge_type == t)\n",
    "                if mask.numel() > 0 and int(mask.sum()) > 0:\n",
    "                    ei = edge_index[:, mask]\n",
    "                    parts.append(conv(h, ei))\n",
    "            h_msg = torch.stack(parts, dim=0).sum(dim=0) if parts else torch.zeros_like(h)\n",
    "        h = self.norm(h + h_msg)\n",
    "        return torch.relu(h)\n",
    "\n",
    "class GGNNClassifierFeatsNoEmb(nn.Module):\n",
    "    def __init__(self, num_types: int, tok_dim: int, small_dim: int = 2,\n",
    "                 steps: int = 10, blocks: int = 5, num_edge_types: int = 3, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.dim_type  = num_types\n",
    "        self.dim_tok   = tok_dim + 1\n",
    "        self.dim_small = small_dim\n",
    "        self.channels  = self.dim_type + self.dim_tok + self.dim_small\n",
    "        self.blocks = nn.ModuleList([GGNNBlockFeats(self.channels, steps, num_edge_types)\n",
    "                                     for _ in range(blocks)])\n",
    "        self.drop   = nn.Dropout(dropout)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.Linear(self.channels, self.channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.channels, 2)\n",
    "        )\n",
    "    def build_features(self, data):\n",
    "        xt = data.x_type.squeeze(-1)\n",
    "        h_type = F.one_hot(xt.long(), num_classes=self.dim_type).float()\n",
    "        xk = data.x_tok.squeeze(-1).clamp(0, self.dim_tok-1).long()\n",
    "        h_tok = F.one_hot(xk, num_classes=self.dim_tok).float()\n",
    "        h_small = data.x_small\n",
    "        return torch.cat([h_type, h_tok, h_small], dim=1)\n",
    "    def forward(self, data):\n",
    "        h = self.build_features(data)\n",
    "        et = getattr(data, \"edge_type\", None)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, data.edge_index, et)\n",
    "            h = self.drop(h)\n",
    "        hg = global_mean_pool(h, data.batch)\n",
    "        return self.head(h)\n",
    "\n",
    "model_ggnn = GGNNClassifierFeatsNoEmb(num_types=vocab_size, tok_dim=TOK_DIM, small_dim=2,\n",
    "                                      steps=10, blocks=5, num_edge_types=num_edge_types, dropout=0.3).to(device)\n",
    "opt = torch.optim.AdamW(model_ggnn.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "crit = nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "\n",
    "def run_epoch(model, loader, train=False):\n",
    "    model.train() if train else model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            if train: opt.zero_grad()\n",
    "            logits = model(batch)\n",
    "            loss = crit(logits, batch.y)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "                opt.step()\n",
    "            loss_sum += loss.item() * batch.num_graphs\n",
    "            pred = logits.argmax(1)\n",
    "            correct += int((pred == batch.y).sum())\n",
    "            total   += batch.num_graphs\n",
    "    return loss_sum/max(1,total), correct/max(1,total)\n",
    "\n",
    "best_val = 0.0; best_state = None\n",
    "for ep in range(1, 31):\n",
    "    tr_loss, tr_acc = run_epoch(model_ggnn, train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(model_ggnn, val_loader,   train=False)\n",
    "    if va_acc > best_val: best_val, best_state = va_acc, model_ggnn.state_dict()\n",
    "    print(f\"[GGNN] epoch {ep:02d} | train {tr_acc:.3f} | val {va_acc:.3f}\")\n",
    "if best_state: model_ggnn.load_state_dict(best_state)\n",
    "_, te_acc = run_epoch(model_ggnn, test_loader, train=False)\n",
    "print(\"[GGNN] TEST acc:\", te_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eefb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "\n",
    "class GINENoEmb(nn.Module):\n",
    "    def __init__(self, num_types:int, tok_dim:int, small_dim:int=2, layers:int=4, dropout:float=0.3, num_edge_types:int=3):\n",
    "        super().__init__()\n",
    "        self.dim_type  = num_types\n",
    "        self.dim_tok   = tok_dim + 1\n",
    "        self.dim_small = small_dim\n",
    "        self.channels  = self.dim_type + self.dim_tok + self.dim_small\n",
    "        def mlp():  # GINE-hez szükséges MLP\n",
    "            return nn.Sequential(nn.Linear(self.channels, self.channels), nn.ReLU(), nn.Linear(self.channels, self.channels))\n",
    "        self.convs = nn.ModuleList([GINEConv(mlp()) for _ in range(layers)])\n",
    "        self.bns   = nn.ModuleList([nn.BatchNorm1d(self.channels) for _ in range(layers)])\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        self.head  = nn.Sequential(nn.Linear(self.channels, self.channels), nn.ReLU(), nn.Dropout(dropout), nn.Linear(self.channels, 2))\n",
    "        self.num_edge_types = num_edge_types\n",
    "    def build_features(self, data):\n",
    "        xt = data.x_type.squeeze(-1)\n",
    "        h_type = F.one_hot(xt.long(), num_classes=self.dim_type).float()\n",
    "        xk = data.x_tok.squeeze(-1).clamp(0, self.dim_tok-1).long()\n",
    "        h_tok = F.one_hot(xk, num_classes=self.dim_tok).float()\n",
    "        h_small = data.x_small\n",
    "        return torch.cat([h_type, h_tok, h_small], dim=1)\n",
    "    def forward(self, data):\n",
    "        h = self.build_features(data)\n",
    "        eattr = F.one_hot(data.edge_type.long(), num_classes=self.num_edge_types).float() if data.edge_type.numel()>0 else None\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h = conv(h, data.edge_index, eattr)\n",
    "            h = bn(h); h = torch.relu(h); h = self.drop(h)\n",
    "        hg = global_mean_pool(h, data.batch)\n",
    "        return self.head(h)\n",
    "\n",
    "model_gine = GINENoEmb(num_types=vocab_size, tok_dim=TOK_DIM, small_dim=2, layers=4, dropout=0.3, num_edge_types=num_edge_types).to(device)\n",
    "opt = torch.optim.AdamW(model_gine.parameters(), lr=1e-3, weight_decay=1e-4)  # új optimizer, külön modellhez\n",
    "\n",
    "best_val = 0.0; best_state = None\n",
    "for ep in range(1, 21):\n",
    "    tr_loss, tr_acc = run_epoch(model_gine, train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(model_gine, val_loader,   train=False)\n",
    "    if va_acc > best_val: best_val, best_state = va_acc, model_gine.state_dict()\n",
    "    print(f\"[GINE] epoch {ep:02d} | train {tr_acc:.3f} | val {va_acc:.3f}\")\n",
    "if best_state: model_gine.load_state_dict(best_state)\n",
    "_, te_acc = run_epoch(model_gine, test_loader, train=False)\n",
    "print(\"[GINE] TEST acc:\", te_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d01fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - szakdoga)",
   "language": "python",
   "name": "szakdoga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
