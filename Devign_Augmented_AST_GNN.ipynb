{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648c12c6",
   "metadata": {},
   "source": [
    "# Devign Augmented AST → PyG → GNN (no-embedding)\\n\n",
    "Baseline: GGNN és GIN/GINE, node feature = type one-hot + token(one-hot, hash) + small([is_leaf, depth_norm])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3e1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, json, hashlib, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_c as tsc\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d040d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree-Sitter C nyelv\n",
    "C_LANG = Language(tsc.language())\n",
    "parser = Parser(C_LANG)\n",
    "\n",
    "@dataclass\n",
    "class ASTGraph:\n",
    "    nodes: List[Dict[str, Any]]\n",
    "    edges: List[Tuple[int, int, str]]  # (src, dst, edge_type)\n",
    "    label: int\n",
    "    raw: str\n",
    "\n",
    "def parse_c(code: str):\n",
    "    return parser.parse(code.encode('utf8'))\n",
    "\n",
    "def build_augmented_ast(tree, source: bytes) -> Tuple[List[Dict[str, Any]], List[Tuple[int, int, str]]]:\n",
    "    nodes, edges = [], []\n",
    "    node_id = 0\n",
    "    def walk(node, parent_id=None, last_sibling_id=None, depth=0):\n",
    "        nonlocal node_id\n",
    "        my_id = node_id; node_id += 1\n",
    "        snippet = source[node.start_byte:node.end_byte]\n",
    "        children = node.children\n",
    "        nodes.append({\n",
    "            'id': my_id,\n",
    "            'type': node.type,\n",
    "            'start_point': node.start_point,\n",
    "            'end_point': node.end_point,\n",
    "            'text': snippet.decode('utf8', 'ignore'),\n",
    "            'is_leaf': int(len(children) == 0),\n",
    "            'depth': depth\n",
    "        })\n",
    "        if parent_id is not None:\n",
    "            edges.append((parent_id, my_id, 'parent'))\n",
    "        if last_sibling_id is not None:\n",
    "            edges.append((last_sibling_id, my_id, 'next_sibling'))\n",
    "        prev_child_id = None\n",
    "        for child in children:\n",
    "            child_id = walk(child, my_id, prev_child_id, depth+1)\n",
    "            if prev_child_id is not None:\n",
    "                edges.append((prev_child_id, child_id, 'next_token'))\n",
    "            prev_child_id = child_id\n",
    "        return my_id\n",
    "    walk(tree.root_node)\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951b2560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'func', 'target', 'project', 'commit_id'],\n",
      "    num_rows: 21854\n",
      "})\n",
      "példa label: False\n",
      "static av_cold int vdadec_init(AVCodecContext *avctx)\n",
      "\n",
      "{\n",
      "\n",
      "    VDADecoderContext *ctx = avctx->priv_data;\n",
      "\n",
      "    struct vda_context *vda_ctx = &ctx->vda_ctx;\n",
      "\n",
      "    OSStatus status;\n",
      "\n",
      "    int ret;\n",
      "\n",
      "\n",
      "\n",
      "    ct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Devign (HF tükör) betöltés\n",
    "ds = load_dataset('google/code_x_glue_cc_defect_detection')\n",
    "train, valid, test = ds['train'], ds.get('validation') or ds.get('valid') or None, ds['test']\n",
    "print(train); print('példa label:', train[0]['target']); print(train[0]['func'][:200])\n",
    "\n",
    "# kiegyensúlyozott random részhalmaz (állítható)\n",
    "N_TRAIN = 2000  # növelhető, ha fér a VRAM/ram\n",
    "pos = train.filter(lambda x: x['target']==1)\n",
    "neg = train.filter(lambda x: x['target']==0)\n",
    "k = min(N_TRAIN//2, len(pos), len(neg))\n",
    "subset = concatenate_datasets([\n",
    "    pos.shuffle(seed=SEED).select(range(k)),\n",
    "    neg.shuffle(seed=SEED+1).select(range(k)),\n",
    "]).shuffle(seed=SEED+2)\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bd0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 143.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 0, 'translation_unit')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ASTGraph-okká alakítás\\n\n",
    "def sample_to_astgraph(sample) -> ASTGraph:\n",
    "    code = sample['func']; label = int(sample['target'])\n",
    "    tree = parse_c(code)\n",
    "    nodes, edges = build_augmented_ast(tree, code.encode('utf8'))\n",
    "    return ASTGraph(nodes=nodes, edges=edges, label=label, raw=code)\n",
    "\n",
    "graphs_ast = [sample_to_astgraph(row) for row in tqdm(subset, total=len(subset))]\n",
    "len(graphs_ast), graphs_ast[0].label, graphs_ast[0].nodes[0]['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15443e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 153.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000,\n",
       " ['translation_unit',\n",
       "  'function_definition',\n",
       "  'storage_class_specifier',\n",
       "  'static',\n",
       "  'primitive_type'],\n",
       " 191)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AugAST -> PyG konverzió (type + token + small)\n",
    "EDGE_TYPES = {'parent':0, 'next_sibling':1, 'next_token':2}\n",
    "TOK_DIM = 1024              # token-hash bucket; sentinel = TOK_DIM\n",
    "TOK_SENTINEL = TOK_DIM\n",
    "\n",
    "def _hash_bucket(s: str, D: int = TOK_DIM) -> int:\n",
    "    if not s or not s.strip():\n",
    "        return TOK_SENTINEL\n",
    "    h = hashlib.md5(s.strip().encode('utf8')).hexdigest()\n",
    "    return int(h, 16) % D\n",
    "\n",
    "def to_pyg(g: ASTGraph) -> Data:\n",
    "    # type vocab (globális)\n",
    "    if not hasattr(to_pyg, 'type_vocab'):\n",
    "        to_pyg.type_vocab = {}\n",
    "    tv = to_pyg.type_vocab\n",
    "\n",
    "    type_ids, tok_ids, small_feats = [], [], []\n",
    "    max_depth = max([n.get('depth',0) for n in g.nodes] + [1])\n",
    "    for n in g.nodes:\n",
    "        t = n['type']\n",
    "        if t not in tv: tv[t] = len(tv)\n",
    "        type_ids.append([tv[t]])\n",
    "        tok = _hash_bucket(n.get('text','') if n.get('is_leaf',0) else '', TOK_DIM)\n",
    "        tok_ids.append([tok])\n",
    "        d = float(n.get('depth',0))/float(max_depth)\n",
    "        small_feats.append([float(n.get('is_leaf',0)), d])\n",
    "\n",
    "    x_type  = torch.tensor(np.array(type_ids),  dtype=torch.long)\n",
    "    x_tok   = torch.tensor(np.array(tok_ids),   dtype=torch.long)\n",
    "    x_small = torch.tensor(np.array(small_feats), dtype=torch.float)\n",
    "\n",
    "    if len(g.edges)==0:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_type  = torch.empty((0,),  dtype=torch.long)\n",
    "    else:\n",
    "        src = [s for (s,_,_) in g.edges]; dst = [d for (_,d,_) in g.edges]\n",
    "        et  = [EDGE_TYPES[t] for (_,_,t) in g.edges]\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "        edge_type  = torch.tensor(et, dtype=torch.long)\n",
    "\n",
    "    data = Data(edge_index=edge_index, y=torch.tensor([g.label], dtype=torch.long))\n",
    "    data.edge_type = edge_type\n",
    "    data.x_type  = x_type\n",
    "    data.x_tok   = x_tok\n",
    "    data.x_small = x_small\n",
    "    data.x = x_type.clone()  # kompat\n",
    "    return data\n",
    "\n",
    "pyg_graphs = [to_pyg(g) for g in tqdm(graphs_ast)]\n",
    "torch.save(pyg_graphs, 'devign_augast_pyg.pt')\n",
    "len(pyg_graphs), list(to_pyg.type_vocab)[:5], len(to_pyg.type_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e13d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: 1600 200 200\n"
     ]
    }
   ],
   "source": [
    "# Betöltés + stratifikált split + DataLoaderek\n",
    "graphs = torch.load('devign_augast_pyg.pt', weights_only=False)\n",
    "ys = np.array([int(g.y.item()) for g in graphs])\n",
    "pos_idx = np.where(ys==1)[0]; neg_idx = np.where(ys==0)[0]\n",
    "rng = np.random.default_rng(SEED)\n",
    "rng.shuffle(pos_idx); rng.shuffle(neg_idx)\n",
    "def split3(idx):\n",
    "    n=len(idx); n_tr=int(0.8*n); n_va=int(0.1*n); return idx[:n_tr], idx[n_tr:n_tr+n_va], idx[n_tr+n_va:]\n",
    "p_tr,p_va,p_te = split3(pos_idx); n_tr,n_va,n_te = split3(neg_idx)\n",
    "train_idx = np.concatenate([p_tr,n_tr]); rng.shuffle(train_idx)\n",
    "val_idx   = np.concatenate([p_va,n_va]); rng.shuffle(val_idx)\n",
    "test_idx  = np.concatenate([p_te,n_te]); rng.shuffle(test_idx)\n",
    "train_set = [graphs[i] for i in train_idx]\n",
    "val_set   = [graphs[i] for i in val_idx]\n",
    "test_set  = [graphs[i] for i in test_idx]\n",
    "class_weight = torch.tensor([1.0, max(1.0, len(n_tr)/max(1,len(p_tr)))], dtype=torch.float)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=128)\n",
    "test_loader  = DataLoader(test_set,  batch_size=128)\n",
    "print('Train/Val/Test:', len(train_set), len(val_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4f28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 191 | TOK_DIM = 1024\n",
      "max xt: 180\n",
      "max xk: 1024\n"
     ]
    }
   ],
   "source": [
    "# Dimenziók kikövetkeztetése + sanity\\n\n",
    "def infer_vocab_size(gs):\n",
    "    mx=0\n",
    "    for g in gs:\n",
    "        if hasattr(g,'x_type') and g.x_type.numel()>0:\n",
    "            mx = max(mx, int(g.x_type.max().item()))\n",
    "        elif hasattr(g,'x') and g.x.numel()>0:\n",
    "            mx = max(mx, int(g.x.max().item()))\n",
    "    return mx+1\n",
    "\n",
    "def infer_tok_dim(gs):\n",
    "    mx=-1\n",
    "    for g in gs:\n",
    "        if hasattr(g,'x_tok') and g.x_tok.numel()>0:\n",
    "            mx = max(mx, int(g.x_tok.max().item()))\n",
    "    return (mx if mx>=0 else TOK_DIM)\n",
    "\n",
    "vocab_size = infer_vocab_size(train_set+val_set+test_set)\n",
    "TOK_DIM_INFER = infer_tok_dim(train_set+val_set+test_set)\n",
    "print('vocab_size =', vocab_size, '| TOK_DIM =', TOK_DIM_INFER)\n",
    "\n",
    "# gyors sanity (CPU-n)\n",
    "batch = next(iter(train_loader))\n",
    "xt = getattr(batch,'x_type', getattr(batch,'x'))\n",
    "xk = getattr(batch,'x_tok', None)\n",
    "print('max xt:', int(xt.max()))\n",
    "print('max xk:', int(xk.max()) if xk is not None and xk.numel()>0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "731e6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== GGNN (no-emb) – type+token+small ====\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GatedGraphConv, global_mean_pool\n",
    "\n",
    "class GGNNBlockFeats(nn.Module):\n",
    "    def __init__(self, channels: int, steps: int, num_edge_types: int = 3):\n",
    "        super().__init__()\n",
    "        self.num_edge_types = max(1, num_edge_types)\n",
    "        self.convs = nn.ModuleList([GatedGraphConv(channels, num_layers=steps) for _ in range(self.num_edge_types)])\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "    def forward(self, h, edge_index, edge_type=None):\n",
    "        if (edge_type is None) or (self.num_edge_types==1):\n",
    "            h_msg = self.convs[0](h, edge_index)\n",
    "        else:\n",
    "            parts=[]\n",
    "            for t,conv in enumerate(self.convs):\n",
    "                mask = (edge_type==t)\n",
    "                if mask.numel()>0 and int(mask.sum())>0:\n",
    "                    ei = edge_index[:, mask]\n",
    "                    parts.append(conv(h, ei))\n",
    "            h_msg = torch.stack(parts, dim=0).sum(dim=0) if parts else torch.zeros_like(h)\n",
    "        h = self.norm(h + h_msg)\n",
    "        return torch.relu(h)\n",
    "\n",
    "class GGNNClassifierFeatsNoEmb(nn.Module):\n",
    "    def __init__(self, num_types:int, tok_dim:int, small_dim:int=2, steps:int=10, blocks:int=5, num_edge_types:int=3, dropout:float=0.3):\n",
    "        super().__init__()\n",
    "        self.dim_type=num_types; self.dim_tok=tok_dim+1; self.dim_small=small_dim\n",
    "        self.channels = self.dim_type + self.dim_tok + self.dim_small\n",
    "        self.blocks = nn.ModuleList([GGNNBlockFeats(self.channels, steps, num_edge_types) for _ in range(blocks)])\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.head = nn.Sequential(nn.Linear(self.channels,self.channels), nn.ReLU(), nn.Dropout(dropout), nn.Linear(self.channels,2))\n",
    "    def build_features(self, data):\n",
    "        xt = getattr(data,'x_type', getattr(data,'x'))\n",
    "        xt = xt.squeeze(-1) if xt.dim()==2 else xt\n",
    "        h_type = F.one_hot(xt.long(), num_classes=self.dim_type).float()\n",
    "        if hasattr(data,'x_tok'):\n",
    "            xk = data.x_tok; xk = xk.squeeze(-1) if xk.dim()==2 else xk\n",
    "            xk = xk.clamp(0, self.dim_tok-1).long()\n",
    "            h_tok = F.one_hot(xk, num_classes=self.dim_tok).float()\n",
    "        else:\n",
    "            N = h_type.size(0)\n",
    "            h_tok = torch.zeros((N,self.dim_tok), dtype=torch.float, device=h_type.device); h_tok[:,self.dim_tok-1]=1.0\n",
    "        h_small = getattr(data,'x_small', torch.zeros((h_type.size(0),self.dim_small), dtype=torch.float, device=h_type.device))\n",
    "        return torch.cat([h_type, h_tok, h_small], dim=1)\n",
    "    def forward(self, data):\n",
    "        h = self.build_features(data)\n",
    "        et = getattr(data,'edge_type', None)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, data.edge_index, et)\n",
    "            h = self.drop(h)\n",
    "        hg = global_mean_pool(h, data.batch)\n",
    "        return self.head(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca04d0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==== GINE (no-emb) – type+token+small + edge_type ====\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "class GINEClassifierFeatsNoEmb(torch.nn.Module):\n",
    "    def __init__(self, num_types:int, tok_dim:int, small_dim:int=2, num_layers:int=4, dropout:float=0.3, num_edge_types:int=3):\n",
    "        super().__init__()\n",
    "        self.dim_type=num_types; self.dim_tok=tok_dim+1; self.dim_small=small_dim\n",
    "        self.channels = self.dim_type + self.dim_tok + self.dim_small\n",
    "        self.num_edge_types = num_edge_types\n",
    "        def mlp():\n",
    "            return torch.nn.Sequential(torch.nn.Linear(self.channels,self.channels), torch.nn.ReLU(), torch.nn.Linear(self.channels,self.channels))\n",
    "        self.gins = torch.nn.ModuleList([GINEConv(mlp()) for _ in range(num_layers)])\n",
    "        self.bns  = torch.nn.ModuleList([torch.nn.BatchNorm1d(self.channels) for _ in range(num_layers)])\n",
    "        self.drop = torch.nn.Dropout(dropout)\n",
    "        self.head = torch.nn.Sequential(torch.nn.Linear(self.channels,self.channels), torch.nn.ReLU(), torch.nn.Dropout(dropout), torch.nn.Linear(self.channels,2))\n",
    "    def build_features(self, data):\n",
    "        xt = getattr(data,'x_type', getattr(data,'x'))\n",
    "        xt = xt.squeeze(-1) if xt.dim()==2 else xt\n",
    "        h_type = F.one_hot(xt.long(), num_classes=self.dim_type).float()\n",
    "        if hasattr(data,'x_tok'):\n",
    "            xk = data.x_tok; xk = xk.squeeze(-1) if xk.dim()==2 else xk\n",
    "            xk = xk.clamp(0, self.dim_tok-1).long()\n",
    "            h_tok = F.one_hot(xk, num_classes=self.dim_tok).float()\n",
    "        else:\n",
    "            N = h_type.size(0)\n",
    "            h_tok = torch.zeros((N,self.dim_tok), dtype=torch.float, device=h_type.device); h_tok[:,self.dim_tok-1]=1.0\n",
    "        h_small = getattr(data,'x_small', torch.zeros((h_type.size(0),self.dim_small), dtype=torch.float, device=h_type.device))\n",
    "        return torch.cat([h_type, h_tok, h_small], dim=1)\n",
    "    def forward(self, data):\n",
    "        h = self.build_features(data)\n",
    "        if hasattr(data,'edge_type') and data.edge_type.numel()>0:\n",
    "            edge_attr = F.one_hot(data.edge_type.long(), num_classes=self.num_edge_types).float()\n",
    "        else:\n",
    "            E = data.edge_index.size(1)\n",
    "            edge_attr = h.new_zeros((E, self.num_edge_types))\n",
    "        for conv, bn in zip(self.gins, self.bns):\n",
    "            h = conv(h, data.edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = torch.relu(h)\n",
    "            h = self.drop(h)\n",
    "        hg = global_mean_pool(h, data.batch)\n",
    "        return self.head(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb76d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Modell választás, init, tréning ====#\n",
    "MODEL = 'ggnn'   # 'ggnn' | 'gine'\n",
    "num_edge_types = 3 if hasattr(train_set[0],'edge_type') and train_set[0].edge_type.numel()>0 else 1\n",
    "tok_dim = TOK_DIM_INFER  # következtetett dimenzió a betöltött gráfokból\n",
    "\n",
    "if MODEL=='ggnn':\n",
    "    model = GGNNClassifierFeatsNoEmb(num_types=vocab_size, tok_dim=tok_dim, small_dim=2, steps=10, blocks=5, num_edge_types=num_edge_types, dropout=0.3).to(device)\n",
    "else:\n",
    "    model = GINEClassifierFeatsNoEmb(num_types=vocab_size, tok_dim=tok_dim, small_dim=2, num_layers=4, dropout=0.3, num_edge_types=num_edge_types).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3 if MODEL!='ggnn' else 3e-4, weight_decay=1e-4)\n",
    "crit = torch.nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "\n",
    "def run(loader, train=False):\n",
    "    model.train() if train else model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        if train: opt.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = crit(logits, batch.y)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            opt.step()\n",
    "        loss_sum += loss.item() * batch.num_graphs\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += int((pred == batch.y).sum())\n",
    "        total   += batch.num_graphs\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(1, 21 if MODEL!='ggnn' else 31):\n",
    "    tr_loss, tr_acc = run(train_loader, train=True)\n",
    "    va_loss, va_acc = run(val_loader,   train=False)\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model.state_dict(), f'best_{MODEL}.pt')\n",
    "    print(f'epoch {epoch:02d} | train {tr_acc:.3f} | val {va_acc:.3f}')\n",
    "\n",
    "model.load_state_dict(torch.load(f'best_{MODEL}.pt', weights_only=False))\n",
    "te_loss, te_acc = run(test_loader, train=False)\n",
    "print('TEST acc:', te_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - szakdoga)",
   "language": "python",
   "name": "szakdoga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
