{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3GNN (C/C++) — Augmented AST (no-embedding) → GGNN / GINE\\n\n",
    "Node feature = **type one-hot + token(one-hot, hash-bucket, levelekből) + small([is_leaf, depth_norm])**.\\n\n",
    "Datasets: **CodeXGLUE (Devign-derived, C)** és **Draper VDISC (C/C++) @ HF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Változók ---\n",
    "from pathlib import Path\n",
    "SELECT_DATASET = 'draper_hf'   # 'code_x_glue' | 'draper_hf'\n",
    "LANG = 'cpp'                  # 'c' | 'cpp'\n",
    "MAX_SAMPLES = 20000           # 0 → mind (óvatosan RAM/VRAM miatt)\n",
    "BATCH_TRAIN, BATCH_EVAL = 64, 128\n",
    "EPOCHS_GGNN, EPOCHS_GINE = 30, 20\n",
    "\n",
    "# token-hash bucket dimenzió (levelek szövegéből)\n",
    "TOK_DIM = 1024                 # 512/1024/2048 – VRAM/gyorsaság kompromisszum\n",
    "TOK_SENTINEL = TOK_DIM         # üres/nem-levél → sentinel id\n",
    "\n",
    "EDGE_TYPES = {'parent':0, 'next_sibling':1, 'next_token':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree-Sitter OK, LANG = cpp\n"
     ]
    }
   ],
   "source": [
    "# --- Tree-Sitter beállítás (C/C++) ---\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_c as tsc\n",
    "import tree_sitter_cpp as tscpp\n",
    "\n",
    "TS_LANG = Language(tsc.language()) if LANG.lower()=='c' else Language(tscpp.language())\n",
    "parser = Parser(TS_LANG)\n",
    "print('Tree-Sitter OK, LANG =', LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minták száma: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s5pcsis_try_format(\\n\\tstruct v4l2_mbus_framef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__ast_dsp_new(unsigned int sample_rate)\\n{\\n\\t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ibus_keymap_load (const gchar *name,\\n        ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  label\n",
       "0  s5pcsis_try_format(\\n\\tstruct v4l2_mbus_framef...      0\n",
       "1  __ast_dsp_new(unsigned int sample_rate)\\n{\\n\\t...      0\n",
       "2  ibus_keymap_load (const gchar *name,\\n        ...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Segédfüggvények: adatbetöltés ---\n",
    "def _normalize_label(x):\n",
    "    if isinstance(x, str):\n",
    "        xs = x.strip().lower()\n",
    "        if xs in {'1','vul','vulnerable','pos','positive','true','bug'}: return 1\n",
    "        if xs in {'0','non-vul','nonvul','benign','safe','neg','negative','false','clean'}: return 0\n",
    "    try:\n",
    "        return 1 if int(x)==1 else 0\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _auto_pick_columns(df: pd.DataFrame):\n",
    "    code_col = None; label_col = None\n",
    "    for c in df.columns:\n",
    "        if c.lower() in {'code','func','function','source','source_code','program'}:\n",
    "            code_col = c; break\n",
    "    for c in df.columns:\n",
    "        if c.lower() in {'label','target','y','vul','vulnerable','bug'}:\n",
    "            label_col = c; break\n",
    "    if code_col is None:\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype==object: code_col = c; break\n",
    "    if label_col is None:\n",
    "        for c in df.columns:\n",
    "            if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_bool_dtype(df[c]):\n",
    "                label_col = c; break\n",
    "    return code_col, label_col\n",
    "\n",
    "def load_any_dataset(select: str) -> pd.DataFrame:\n",
    "    s = select.lower()\n",
    "    if s == 'code_x_glue':\n",
    "        ds = load_dataset('google/code_x_glue_cc_defect_detection')\n",
    "        df = pd.DataFrame({'code': ds['train']['func'], 'label': ds['train']['target']})\n",
    "        df['label'] = df['label'].apply(_normalize_label)\n",
    "        return df.dropna(subset=['code','label']).reset_index(drop=True)\n",
    "    elif s == 'draper_hf':\n",
    "        ds = load_dataset('claudios/Draper')  # HF tükör; mezők változhatnak\n",
    "        split = 'train' if 'train' in ds else list(ds.keys())[0]\n",
    "        df = pd.DataFrame({c: ds[split][c] for c in ds[split].column_names})\n",
    "        ccol, lcol = _auto_pick_columns(df)\n",
    "        df = df[[ccol, lcol]].rename(columns={ccol:'code', lcol:'label'})\n",
    "        df['label'] = df['label'].apply(_normalize_label)\n",
    "        return df.dropna(subset=['code','label']).reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(select)\n",
    "\n",
    "raw_df = load_any_dataset(SELECT_DATASET)\n",
    "if MAX_SAMPLES and len(raw_df) > MAX_SAMPLES:\n",
    "    raw_df = raw_df.sample(MAX_SAMPLES, random_state=SEED).reset_index(drop=True)\n",
    "raw_df['label'] = raw_df['label'].astype(int)\n",
    "print('Minták száma:', len(raw_df))\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21f55953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['functionSource', 'CWE-119', 'CWE-120', 'CWE-469', 'CWE-476', 'CWE-other', 'combine']\n",
      "\n",
      "functionSource top values:\n",
      "functionSource\n",
      "clear_area(int startx, int starty, int xsize, int ysize)\\n{\\n  int x;\\n\\n  TRACE_LOG(\"Clearing area %d,%d / %d,%d\\n\", startx, starty, xsize, ysize);\\n\\n  while (ysize > 0)\\n  {\\n    x = xsize;\\n    while (x > 0)\\n    {\\n      mvaddch(starty + ysize - 2, startx + x - 2, ' ');\\n      x--;\\n    }\\n    ysize--;\\n  }\\n}                                                                                                                                                                                                                                                  1\n",
      "ReconstructDuList(Statement* head)\\n{\\n    Statement* spt;\\n\\n    for (spt = head; spt != NULL; spt = spt->next) {\\n\\tdelete_def_use_list(spt->use_var_list);\\n\\tdelete_def_use_list(spt->def_var_list);\\n\\tdelete_def_use_list(spt->use_array_list);\\n\\tdelete_def_use_list(spt->def_array_list);\\n\\tspt->def_var_list = NULL;\\n\\tspt->use_var_list = NULL;\\n\\tspt->def_array_list = NULL;\\n\\tspt->use_array_list = NULL;\\n    }\\n    def_use_statement(head);\\n}                                                                                                            1\n",
      "free_speaker(void)\\n{\\n   if(Lengths)\\n       free(Lengths);\\n\\n   if(!audio2fast && commento)\\n       fclose(commento);\\n\\n\\n   frase = NON_DECISA;\\n   game_status = S_NON_INIZIATO;\\n\\n   fondolen = sound[FONDO]->Length;\\n   fondobase = sound[FONDO]->SoundData;\\n\\n   if (audio2fast && comment_file) \\n       free(comment_file);\\n   \\n   Lengths = NULL;\\n   commento = NULL;\\n   comment_file = NULL;\\n}                                                                                                                                                           1\n",
      "mlx4_register_device(struct mlx4_dev *dev)\\n{\\n\\tstruct mlx4_priv *priv = mlx4_priv(dev);\\n\\tstruct mlx4_interface *intf;\\n\\n\\tmutex_lock(&intf_mutex);\\n\\n\\tdev->persist->interface_state |= MLX4_INTERFACE_STATE_UP;\\n\\tlist_add_tail(&priv->dev_list, &dev_list);\\n\\tlist_for_each_entry(intf, &intf_list, list)\\n\\t\\tmlx4_add_device(intf, priv);\\n\\n\\tmutex_unlock(&intf_mutex);\\n\\tmlx4_start_catas_poll(dev);\\n\\n\\treturn 0;\\n}                                                                                                                                        1\n",
      "Parse_Env_Var(void)\\n{\\n  char *p = getenv(\"LINEDIT\");\\n\\n  if (p == NULL)\\n    return;\\n\\n  if (strstr(p, \"gui=no\") != NULL)\\n    use_gui = 0;\\n      \\n  if (strstr(p, \"ansi=no\") != NULL)\\n    use_ansi = 0;\\n\\n  if ((p = strstr(p, \"out=\")) != NULL)\\n    {\\n      p += 4;\\n\\n      if (isdigit(*p))\\n\\tfd_out = strtol(p, NULL, 10);\\n      else\\n\\t{\\n\\t  char buff[1024];\\n\\t  char *q = buff;\\n\\n\\t  while(*p && isprint(*p) && !isspace(*p))\\n\\t    *q++ = *p++;\\n\\n\\t  *q = '\\0';\\n\\t  fd_out = open(buff, O_WRONLY); /* on error fd_out = -1 */\\n\\t}\\n    }\\n}    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CWE-119 top values:\n",
      "CWE-119\n",
      "False    1000185\n",
      "True       19286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CWE-120 top values:\n",
      "CWE-120\n",
      "False    981452\n",
      "True      38019\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CWE-469 top values:\n",
      "CWE-469\n",
      "False    1017376\n",
      "True        2095\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CWE-476 top values:\n",
      "CWE-476\n",
      "False    1009777\n",
      "True        9694\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CWE-other top values:\n",
      "CWE-other\n",
      "False    991512\n",
      "True      27959\n",
      "Name: count, dtype: int64\n",
      "\n",
      "combine top values:\n",
      "combine\n",
      "0    953567\n",
      "1     65904\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mi van ténylegesen a HF-tükörben?\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset('claudios/Draper')\n",
    "split = 'train' if 'train' in ds else list(ds.keys())[0]\n",
    "df_raw = pd.DataFrame({c: ds[split][c] for c in ds[split].column_names})\n",
    "print(df_raw.columns.tolist())\n",
    "for c in df_raw.columns:\n",
    "    vc = pd.Series(df_raw[c]).value_counts(dropna=False).head()\n",
    "    print(f\"\\n{c} top values:\\n{vc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 16000 | arány:\n",
      " label\n",
      "0    0.981\n",
      "1    0.019\n",
      "Name: proportion, dtype: float64\n",
      "Val: 2000 | arány:\n",
      " label\n",
      "0    0.981\n",
      "1    0.019\n",
      "Name: proportion, dtype: float64\n",
      "Test: 2000 | arány:\n",
      " label\n",
      "0    0.982\n",
      "1    0.018\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Stratifikált split (0.8/0.1/0.1) ---\n",
    "df_train, df_tmp = train_test_split(raw_df, test_size=0.2, stratify=raw_df['label'], random_state=SEED)\n",
    "df_val, df_test  = train_test_split(df_tmp,   test_size=0.5, stratify=df_tmp['label'], random_state=SEED)\n",
    "for name, df in [(\"Train\", df_train), (\"Val\", df_val), (\"Test\", df_test)]:\n",
    "    print(f\"{name}: {len(df)} | arány:\\n\", df['label'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [00:26<00:00, 605.48it/s] \n",
      "100%|██████████| 2000/2000 [00:03<00:00, 562.66it/s]\n",
      "100%|██████████| 2000/2000 [00:03<00:00, 610.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16000, 2000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Augmented AST építés (C/C++) ---\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class ASTGraph:\n",
    "    nodes: List[Dict[str, Any]]\n",
    "    edges: List[Tuple[int, int, str]]\n",
    "    label: int\n",
    "    raw: str\n",
    "\n",
    "def build_augmented_ast(code: str):\n",
    "    tree = parser.parse(code.encode('utf8'))\n",
    "    nodes, edges = [], []\n",
    "    nid = 0\n",
    "    def walk(node, parent_id=None, last_sib=None, depth=0):\n",
    "        nonlocal nid\n",
    "        my = nid; nid += 1\n",
    "        snippet = code.encode('utf8')[node.start_byte:node.end_byte]\n",
    "        children = node.children\n",
    "        nodes.append({\n",
    "            'id': my,\n",
    "            'type': node.type,\n",
    "            'is_leaf': int(len(children)==0),\n",
    "            'depth': depth,\n",
    "            'text': snippet.decode('utf8','ignore')\n",
    "        })\n",
    "        if parent_id is not None: edges.append((parent_id, my, 'parent'))\n",
    "        if last_sib   is not None: edges.append((last_sib,   my, 'next_sibling'))\n",
    "        prev = None\n",
    "        for ch in children:\n",
    "            ch_id = walk(ch, my, prev, depth+1)\n",
    "            if prev is not None: edges.append((prev, ch_id, 'next_token'))\n",
    "            prev = ch_id\n",
    "        return my\n",
    "    walk(tree.root_node)\n",
    "    return nodes, edges\n",
    "\n",
    "def df_to_graphs(df: pd.DataFrame):\n",
    "    out = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        code, y = str(row['code']), int(row['label'])\n",
    "        n,e = build_augmented_ast(code)\n",
    "        out.append(ASTGraph(n,e,y,code))\n",
    "    return out\n",
    "\n",
    "graphs_train = df_to_graphs(df_train)\n",
    "graphs_val   = df_to_graphs(df_val)\n",
    "graphs_test  = df_to_graphs(df_test)\n",
    "len(graphs_train), len(graphs_val), len(graphs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyG gráfok: 16000 2000 2000 | vocab_size = 263\n"
     ]
    }
   ],
   "source": [
    "# --- PyG konverzió: type + token (levelek szövege hash-bucket) + small ---\n",
    "class Vocab:\n",
    "    def __init__(self): self.map = {}\n",
    "    def id(self, k):\n",
    "        if k not in self.map: self.map[k] = len(self.map)\n",
    "        return self.map[k]\n",
    "    def size(self): return len(self.map)\n",
    "\n",
    "type_vocab = Vocab()\n",
    "\n",
    "def _hash_bucket(s: str, D: int = TOK_DIM) -> int:\n",
    "    if not s or not s.strip():\n",
    "        return TOK_SENTINEL\n",
    "    h = hashlib.md5(s.strip().encode('utf8')).hexdigest()\n",
    "    return int(h, 16) % D\n",
    "\n",
    "def to_pyg(gs):\n",
    "    pyg = []\n",
    "    for g in gs:\n",
    "        # type id\\n\n",
    "        type_ids = [[type_vocab.id(n['type'])] for n in g.nodes]\n",
    "        x_type = torch.tensor(np.array(type_ids), dtype=torch.long)\n",
    "        # token bucket csak levelek text-jéből\n",
    "        tok_ids = [[_hash_bucket(n.get('text','') if n.get('is_leaf',0) else '', TOK_DIM)] for n in g.nodes]\n",
    "        x_tok = torch.tensor(np.array(tok_ids), dtype=torch.long)\n",
    "        # small numerikus: [is_leaf, depth_norm]\n",
    "        max_depth = max([n.get('depth',0) for n in g.nodes] + [1])\n",
    "        small = [[float(n.get('is_leaf',0)), float(n.get('depth',0))/float(max_depth)] for n in g.nodes]\n",
    "        x_small = torch.tensor(np.array(small), dtype=torch.float)\n",
    "        # élek\n",
    "        if len(g.edges)==0:\n",
    "            edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "            edge_type  = torch.empty((0,), dtype=torch.long)\n",
    "        else:\n",
    "            src = [s for s,_,_ in g.edges]\n",
    "            dst = [d for _,d,_ in g.edges]\n",
    "            et  = [EDGE_TYPES[t] for *_,t in g.edges]\n",
    "            edge_index = torch.tensor([src,dst], dtype=torch.long)\n",
    "            edge_type  = torch.tensor(et, dtype=torch.long)\n",
    "        data = Data(edge_index=edge_index, y=torch.tensor([g.label], dtype=torch.long))\n",
    "        data.edge_type = edge_type\n",
    "        data.x_type  = x_type\n",
    "        data.x_tok   = x_tok\n",
    "        data.x_small = x_small\n",
    "        data.x = x_type.clone()  # kompat hibák ellen\n",
    "        pyg.append(data)\n",
    "    return pyg\n",
    "\n",
    "pyg_train = to_pyg(graphs_train)\n",
    "pyg_val   = to_pyg(graphs_val)\n",
    "pyg_test  = to_pyg(graphs_test)\n",
    "vocab_size = type_vocab.size()\n",
    "print('PyG gráfok:', len(pyg_train), len(pyg_val), len(pyg_test), '| vocab_size =', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: [1.0, 52.5117073059082]\n",
      "max type id: 203 < vocab_size = 263\n",
      "max token id: 1024 <= TOK_DIM = 1024\n"
     ]
    }
   ],
   "source": [
    "# --- DataLoaderek + osztálysúly ---\n",
    "train_loader = DataLoader(pyg_train, batch_size=BATCH_TRAIN, shuffle=True)\n",
    "val_loader   = DataLoader(pyg_val,   batch_size=BATCH_EVAL)\n",
    "test_loader  = DataLoader(pyg_test,  batch_size=BATCH_EVAL)\n",
    "\n",
    "y_train = np.array([int(g.y.item()) for g in pyg_train])\n",
    "pos = (y_train==1).sum(); neg = (y_train==0).sum()\n",
    "class_weight = torch.tensor([1.0, max(1.0, neg/max(1,pos))], dtype=torch.float)\n",
    "print('Class weight:', class_weight.tolist())\n",
    "\n",
    "# Sanity — indexek férjenek bele\n",
    "batch = next(iter(train_loader))\n",
    "xt = getattr(batch, 'x_type', getattr(batch, 'x'))\n",
    "xk = getattr(batch, 'x_tok', None)\n",
    "print('max type id:', int(xt.max()), '< vocab_size =', vocab_size)\n",
    "if xk is not None and xk.numel()>0:\n",
    "    print('max token id:', int(xk.max()), '<= TOK_DIM =', TOK_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GGNN (no-embedding) ---\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GatedGraphConv, global_mean_pool\n",
    "\n",
    "class GGNNBlockFeats(nn.Module):\n",
    "    def __init__(self, channels: int, steps: int, num_edge_types: int = 3):\n",
    "        super().__init__()\n",
    "        self.num_edge_types = max(1, num_edge_types)\n",
    "        self.convs = nn.ModuleList([GatedGraphConv(channels, num_layers=steps) for _ in range(self.num_edge_types)])\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "    def forward(self, h, edge_index, edge_type=None):\n",
    "        if (edge_type is None) or (self.num_edge_types==1):\n",
    "            h_msg = self.convs[0](h, edge_index)\n",
    "        else:\n",
    "            parts=[]\n",
    "            for t, conv in enumerate(self.convs):\n",
    "                mask = (edge_type==t)\n",
    "                if mask.numel()>0 and int(mask.sum())>0:\n",
    "                    ei = edge_index[:, mask]\n",
    "                    parts.append(conv(h, ei))\n",
    "            h_msg = torch.stack(parts, dim=0).sum(dim=0) if parts else torch.zeros_like(h)\n",
    "        h = self.norm(h + h_msg)\n",
    "        return torch.relu(h)\n",
    "\n",
    "class GGNNClassifierFeatsNoEmb(nn.Module):\n",
    "    def __init__(self, num_types:int, tok_dim:int, small_dim:int=2, steps:int=10, blocks:int=5, num_edge_types:int=3, dropout:float=0.3):\n",
    "        super().__init__()\n",
    "        self.dim_type=num_types; self.dim_tok=tok_dim+1; self.dim_small=small_dim\n",
    "        self.channels = self.dim_type + self.dim_tok + self.dim_small\n",
    "        self.blocks = nn.ModuleList([GGNNBlockFeats(self.channels, steps, num_edge_types) for _ in range(blocks)])\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.head = nn.Sequential(nn.Linear(self.channels,self.channels), nn.ReLU(), nn.Dropout(dropout), nn.Linear(self.channels,2))\n",
    "    def build_features(self, data):\n",
    "        xt = getattr(data,'x_type', getattr(data,'x'))\n",
    "        xt = xt.squeeze(-1) if xt.dim()==2 else xt\n",
    "        h_type = F.one_hot(xt.long(), num_classes=self.dim_type).float()\n",
    "        if hasattr(data,'x_tok'):\n",
    "            xk = data.x_tok; xk = xk.squeeze(-1) if xk.dim()==2 else xk\n",
    "            xk = xk.clamp(0, self.dim_tok-1).long()\n",
    "            h_tok = F.one_hot(xk, num_classes=self.dim_tok).float()\n",
    "        else:\n",
    "            N = h_type.size(0)\n",
    "            h_tok = torch.zeros((N,self.dim_tok), dtype=torch.float, device=h_type.device)\n",
    "            h_tok[:, self.dim_tok-1] = 1.0\n",
    "        h_small = getattr(data,'x_small', torch.zeros((h_type.size(0),self.dim_small), dtype=torch.float, device=h_type.device))\n",
    "        return torch.cat([h_type, h_tok, h_small], dim=1)\n",
    "    def forward(self, data):\n",
    "        h = self.build_features(data)\n",
    "        et = getattr(data,'edge_type', None)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, data.edge_index, et)\n",
    "            h = self.drop(h)\n",
    "        hg = global_mean_pool(h, data.batch)\n",
    "        return self.head(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GINE (GIN + edge_attr, no-embedding) ---\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "\n",
    "class GINEClassifierFeatsNoEmb(torch.nn.Module):\n",
    "    def __init__(self, num_types:int, tok_dim:int, small_dim:int=2, num_layers:int=4, dropout:float=0.3, num_edge_types:int=3):\n",
    "        super().__init__()\n",
    "        self.dim_type=num_types; self.dim_tok=tok_dim+1; self.dim_small=small_dim\n",
    "        self.channels = self.dim_type + self.dim_tok + self.dim_small\n",
    "        self.num_edge_types = num_edge_types\n",
    "        def mlp():\n",
    "            return torch.nn.Sequential(torch.nn.Linear(self.channels,self.channels), torch.nn.ReLU(), torch.nn.Linear(self.channels,self.channels))\n",
    "        self.gins = torch.nn.ModuleList([GINEConv(mlp()) for _ in range(num_layers)])\n",
    "        self.bns  = torch.nn.ModuleList([torch.nn.BatchNorm1d(self.channels) for _ in range(num_layers)])\n",
    "        self.drop = torch.nn.Dropout(dropout)\n",
    "        self.head = torch.nn.Sequential(torch.nn.Linear(self.channels,self.channels), torch.nn.ReLU(), torch.nn.Dropout(dropout), torch.nn.Linear(self.channels,2))\n",
    "    def build_features(self, data):\n",
    "        xt = getattr(data,'x_type', getattr(data,'x'))\n",
    "        xt = xt.squeeze(-1) if xt.dim()==2 else xt\n",
    "        h_type = F.one_hot(xt.long(), num_classes=self.dim_type).float()\n",
    "        if hasattr(data,'x_tok'):\n",
    "            xk = data.x_tok; xk = xk.squeeze(-1) if xk.dim()==2 else xk\n",
    "            xk = xk.clamp(0, self.dim_tok-1).long()\n",
    "            h_tok = F.one_hot(xk, num_classes=self.dim_tok).float()\n",
    "        else:\n",
    "            N = h_type.size(0)\n",
    "            h_tok = torch.zeros((N,self.dim_tok), dtype=torch.float, device=h_type.device); h_tok[:,self.dim_tok-1]=1.0\n",
    "        h_small = getattr(data,'x_small', torch.zeros((h_type.size(0),self.dim_small), dtype=torch.float, device=h_type.device))\n",
    "        return torch.cat([h_type, h_tok, h_small], dim=1)\n",
    "    def forward(self, data):\n",
    "        h = self.build_features(data)\n",
    "        if hasattr(data,'edge_type') and data.edge_type.numel()>0:\n",
    "            edge_attr = F.one_hot(data.edge_type.long(), num_classes=self.num_edge_types).float()\n",
    "        else:\n",
    "            E = data.edge_index.size(1)\n",
    "            edge_attr = h.new_zeros((E, self.num_edge_types))\n",
    "        for conv, bn in zip(self.gins, self.bns):\n",
    "            h = conv(h, data.edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = torch.relu(h)\n",
    "            h = self.drop(h)\n",
    "        hg = global_mean_pool(h, data.batch)\n",
    "        return self.head(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Modell választó + tréning/eval ---\n",
    "MODEL = 'ggnn'   # 'ggnn' | 'gine'\n",
    "num_edge_types = len(EDGE_TYPES)\n",
    "\n",
    "if MODEL=='ggnn':\n",
    "    model = GGNNClassifierFeatsNoEmb(num_types=vocab_size, tok_dim=TOK_DIM, small_dim=2,\n",
    "                                     steps=10, blocks=5, num_edge_types=num_edge_types, dropout=0.3).to(device)\n",
    "    lr, epochs = 3e-4, EPOCHS_GGNN\n",
    "else:\n",
    "    model = GINEClassifierFeatsNoEmb(num_types=vocab_size, tok_dim=TOK_DIM, small_dim=2,\n",
    "                                     num_layers=4, dropout=0.3, num_edge_types=num_edge_types).to(device)\n",
    "    lr, epochs = 1e-3, EPOCHS_GINE\n",
    "\n",
    "opt  = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "crit = torch.nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "\n",
    "def run(loader, train=False):\n",
    "    model.train() if train else model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        if train: opt.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = crit(logits, batch.y)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            opt.step()\n",
    "        loss_sum += loss.item() * batch.num_graphs\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += int((pred == batch.y).sum())\n",
    "        total   += batch.num_graphs\n",
    "    return (loss_sum/total if total else 0.0), (correct/total if total else 0.0)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval(); y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch)\n",
    "            y_true += batch.y.cpu().tolist()\n",
    "            y_pred += logits.argmax(1).cpu().tolist()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return acc, prec, rec, f1, cm\n",
    "\n",
    "best_val, best_state = 0.0, None\n",
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss, tr_acc = run(train_loader, train=True)\n",
    "    va_acc, va_prec, va_rec, va_f1, _ = evaluate(val_loader)\n",
    "    if va_acc > best_val: best_val, best_state = va_acc, model.state_dict()\n",
    "    print(f\"epoch {epoch:02d} | train acc {tr_acc:.3f} | val acc {va_acc:.3f} | val F1 {va_f1:.3f}\")\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "te_acc, te_prec, te_rec, te_f1, te_cm = evaluate(test_loader)\n",
    "print(\"TEST | acc:\", te_acc, \"| prec:\", te_prec, \"| rec:\", te_rec, \"| f1:\", te_f1)\n",
    "print(\"Confusion matrix:\\n\", te_cm)\n",
    "torch.save(model.state_dict(), f'cpp_augast_{MODEL}_best.pt')\n",
    "print('Mentve:', f'cpp_augast_{MODEL}_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cd240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv - szakdoga)",
   "language": "python",
   "name": "szakdoga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
